{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felipednegredo/tcc-emocoes-musicais-codigo/blob/main/TCC_Fingerprint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqPfD0_pzhK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3afdd16d-85e7-4405-9a94-6bbee3fed728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q -U kaleido pyarrow fastparquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iypdWmJI2i_e"
      },
      "source": [
        "## Teste 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cYh_395XtyU"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "#  FINGERPRINT em LOTE (blocks_by_song/*.parquet) ‚Äî Shazam-like\n",
        "#  - STFT (detec√ß√£o de dB autom√°tico) + contraste est√°vel\n",
        "#  - Picos: densidade alvo + TOP-K por banda + equaliza√ß√£o opcional\n",
        "#  - Fan-out: √¢ncora + linhas (sem Target Zone)  [sem c√≠rculo]\n",
        "#  - Exporta: HTML por bloco e PARQUET com picos por m√∫sica\n",
        "#  - Fundo branco, ticks de X controlados, barras de progresso\n",
        "#  - Ignora blocos com valence/arousal vazios\n",
        "# ===============================================================\n",
        "\n",
        "import os, re, io, zlib, math, time, warnings, pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import librosa\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ------------- tqdm (progress bar) -------------\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except Exception:\n",
        "    tqdm = None\n",
        "\n",
        "try:\n",
        "    from scipy.ndimage import median_filter\n",
        "    SCIPY_OK = True\n",
        "except Exception:\n",
        "    SCIPY_OK = False\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "# Aceita diret√≥rio (varre *.parquet) ou um √∫nico arquivo\n",
        "IN_PATH       = \"/content/drive/MyDrive/DataSet TCC/DEAM/parquet/blocks_by_song\"\n",
        "OUT_ROOT      = \"/content/drive/MyDrive/DataSet TCC/DEAM/fingerprint\"\n",
        "\n",
        "# Renderiza√ß√£o (ambas independentes)\n",
        "DO_DUAL       = True     # STFT | Constellation\n",
        "DO_OVERLAY    = True     # STFT + Constellation\n",
        "\n",
        "EXPORT_HTML   = True     # exporta HTML (sempre dispon√≠vel)\n",
        "IMG_W, IMG_H  = 1280, 640\n",
        "\n",
        "# Visual\n",
        "YLIM_HZ       = (50, 8000)\n",
        "USE_LOG_Y     = False\n",
        "SPEC_CMAP     = \"Inferno\"\n",
        "ZLIM_FIXED    = (-75, 0)\n",
        "TITLE_PREFIX  = \"Fingerprint espectral ‚Äî \"\n",
        "\n",
        "# Picos (bandas + TOP-K)\n",
        "BANDS              = [(80,300), (300,1200), (1200,4000), (4000,8000)]\n",
        "TOPK_PER_BAND      = 1\n",
        "\n",
        "# Equaliza√ß√£o para detec√ß√£o (opcional)\n",
        "EQUALIZE_FOR_PEAKS = True\n",
        "EQ_SIZE_FREQ       = 15\n",
        "\n",
        "# Controle de densidade (busca bin√°ria no percentil global)\n",
        "AUTO_PEAK_DENSITY     = True\n",
        "TARGET_PEAKS_PER_SEC  = 20.0\n",
        "PCTL_SEARCH_RANGE     = (80.0, 99.9)\n",
        "MAX_BINSEARCH_ITERS   = 12\n",
        "TOLERANCE_RELATIVE    = 0.20\n",
        "\n",
        "# Fan-out (√¢ncora central + linhas) ‚Äî sem c√≠rculo\n",
        "SHOW_FAN_LINES  = True\n",
        "FAN_T_MIN       = 0.02\n",
        "FAN_T_MAX       = 0.50\n",
        "FAN_FANOUT      = 6\n",
        "\n",
        "# Eixo X (ticks em passo fixo)\n",
        "X_TICK_STEP_BASE = 2.0  # segundos\n",
        "\n",
        "# S√≠mbolo dos picos: **bolinhas** (sem ‚ÄúX‚Äù)\n",
        "PK_MARKER = dict(\n",
        "    symbol=\"circle\", size=6, opacity=0.95,\n",
        "    color=\"#000000\", line=dict(width=0.8, color=\"#111111\")\n",
        ")\n",
        "\n",
        "# =================== HELPERS ====================\n",
        "def ensure_dir(p): pathlib.Path(p).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def slugify(s):\n",
        "    s = (s or \"desconhecida\").strip().lower()\n",
        "    s = re.sub(r\"[^\\w\\s-]+\", \"\", s); s = re.sub(r\"\\s+\", \"_\", s)\n",
        "    return s[:60] or \"sem_titulo\"\n",
        "\n",
        "def bytes_to_ndarray(b):\n",
        "    if b is None: return np.array([])\n",
        "    if isinstance(b, (bytes, bytearray)):\n",
        "        try:\n",
        "            raw = zlib.decompress(b)\n",
        "            return np.load(io.BytesIO(raw), allow_pickle=False)\n",
        "        except Exception:\n",
        "            return np.load(io.BytesIO(b), allow_pickle=False)\n",
        "    if isinstance(b, np.ndarray): return b\n",
        "    return np.array([])\n",
        "\n",
        "def _nice_tick_params(times: np.ndarray):\n",
        "    if times is None or times.size == 0:\n",
        "        return dict(dtick=X_TICK_STEP_BASE, tick0=0.0)\n",
        "    t0 = float(np.nanmin(times)); t1 = float(np.nanmax(times))\n",
        "    dur = max(t1 - t0, 0.0)\n",
        "    step = X_TICK_STEP_BASE\n",
        "    if dur > 60:   step = 5 * X_TICK_STEP_BASE\n",
        "    if dur > 300:  step = 10 * X_TICK_STEP_BASE\n",
        "    tick0 = math.floor(t0 / step) * step\n",
        "    return dict(dtick=step, tick0=tick0)\n",
        "\n",
        "def detect_scale_and_fix(S, freqs, times, zlim_fixed=ZLIM_FIXED):\n",
        "    if S is None or S.size == 0:\n",
        "        raise ValueError(\"detect_scale_and_fix: matriz S vazia.\")\n",
        "    S = np.array(S, copy=False)\n",
        "\n",
        "    # Orienta√ß√£o\n",
        "    try:\n",
        "        if freqs.size and times.size and S.shape[0] == times.size and S.shape[1] == freqs.size:\n",
        "            S = S.T\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Sanitiza√ß√£o\n",
        "    S = np.where(np.isfinite(S), S, np.nan)\n",
        "    if np.isnan(S).all():\n",
        "        raise ValueError(\"detect_scale_and_fix: S s√≥ cont√©m NaN/Inf.\")\n",
        "    finite_min = float(np.nanmin(S))\n",
        "    S = np.nan_to_num(S, nan=finite_min)\n",
        "\n",
        "    # Escala\n",
        "    S_min, S_max = float(np.min(S)), float(np.max(S))\n",
        "    frac_neg = float((S < 0).mean()) if S.size else 0.0\n",
        "    is_db_like = ((S_max <= 5.0 and S_min < -10.0) or (frac_neg > 0.6))\n",
        "\n",
        "    if np.iscomplexobj(S):\n",
        "        S_abs = np.abs(S); is_db_like = False\n",
        "    else:\n",
        "        S_abs = np.abs(S)\n",
        "\n",
        "    if is_db_like:\n",
        "        S_db = S.astype(float, copy=False)\n",
        "    else:\n",
        "        ref = np.max(S_abs) if np.max(S_abs) > 0 else 1.0\n",
        "        S_db = librosa.amplitude_to_db(S_abs, ref=ref)\n",
        "\n",
        "    # freqs/times default\n",
        "    if freqs is None or (isinstance(freqs, np.ndarray) and freqs.size == 0):\n",
        "        freqs = np.linspace(0.0, 22050.0, S_db.shape[0], dtype=float)\n",
        "    if times is None or (isinstance(times, np.ndarray) and times.size == 0):\n",
        "        times = np.arange(S_db.shape[1], dtype=float)\n",
        "\n",
        "    # z-limits\n",
        "    if zlim_fixed is not None and len(zlim_fixed) == 2:\n",
        "        zmin, zmax = zlim_fixed\n",
        "    else:\n",
        "        zmin, zmax = np.nanpercentile(S_db, [1, 99])\n",
        "    return S_db, np.asarray(freqs, float), np.asarray(times, float), (float(zmin), float(zmax))\n",
        "\n",
        "def equalize_for_peaks(S_db):\n",
        "    if not SCIPY_OK or EQ_SIZE_FREQ <= 1: return S_db\n",
        "    return S_db - median_filter(S_db, size=(EQ_SIZE_FREQ, 1))\n",
        "\n",
        "# -------- picos por percentil FIXO (base p/ busca bin√°ria) --------\n",
        "def _peaks_with_percentile(S_db, freqs, times, pctl):\n",
        "    if S_db is None or S_db.size == 0:\n",
        "        return np.array([]), np.array([])\n",
        "    freqs = np.asarray(freqs) if isinstance(freqs, np.ndarray) else np.array(freqs)\n",
        "    times = np.asarray(times) if isinstance(times, np.ndarray) else np.array(times)\n",
        "    if freqs.size == 0:\n",
        "        freqs = np.linspace(0.0, 22050.0, S_db.shape[0], dtype=float)\n",
        "    if times.size == 0:\n",
        "        times = np.arange(S_db.shape[1], dtype=float)\n",
        "\n",
        "    n_freq, n_frames = S_db.shape\n",
        "    pk_t, pk_f = [], []\n",
        "\n",
        "    S_for = equalize_for_peaks(S_db) if EQUALIZE_FOR_PEAKS else S_db\n",
        "    thr_cols = np.percentile(S_for, pctl, axis=0)  # shape (n_frames,)\n",
        "\n",
        "    for t in range(n_frames):\n",
        "        col = S_for[:, t]\n",
        "        thr = float(thr_cols[t])\n",
        "        for (lo, hi) in BANDS:\n",
        "            band = (freqs >= lo) & (freqs < hi)\n",
        "            idxs = np.where((col >= thr) & band)[0]\n",
        "            if idxs.size == 0:\n",
        "                continue\n",
        "            sel = idxs[np.argsort(col[idxs])[::-1][:TOPK_PER_BAND]]\n",
        "            pk_t.extend([times[t]] * len(sel))\n",
        "            pk_f.extend(freqs[sel].tolist())\n",
        "\n",
        "    return np.asarray(pk_t), np.asarray(pk_f)\n",
        "\n",
        "# -------- picos com densidade-alvo --------\n",
        "def estimate_peaks_auto_density(S_db, freqs, times,\n",
        "                                target_pps=TARGET_PEAKS_PER_SEC):\n",
        "    if S_db is None or S_db.size == 0:\n",
        "        return np.array([]), np.array([])\n",
        "    freqs = np.asarray(freqs) if isinstance(freqs, np.ndarray) else np.array(freqs)\n",
        "    times = np.asarray(times) if isinstance(times, np.ndarray) else np.array(times)\n",
        "    if times.size == 0:\n",
        "        times = np.arange(S_db.shape[1], dtype=float)\n",
        "\n",
        "    dur = max(float(np.nanmax(times) - np.nanmin(times)), 1e-6)\n",
        "    target_total = int(target_pps * dur)\n",
        "\n",
        "    lo, hi = PCTL_SEARCH_RANGE\n",
        "    best_pt, best_pf = np.array([]), np.array([]); best_diff = float(\"inf\")\n",
        "\n",
        "    for _ in range(MAX_BINSEARCH_ITERS):\n",
        "        mid = (lo + hi) / 2.0\n",
        "        pt, pf = _peaks_with_percentile(S_db, freqs, times, pctl=mid)\n",
        "        n = int(pt.size)\n",
        "        diff = abs(n - target_total)\n",
        "        if diff < best_diff:\n",
        "            best_diff, best_pt, best_pf = diff, pt, pf\n",
        "        if target_total == 0 or abs(n - target_total) / max(target_total, 1) <= TOLERANCE_RELATIVE:\n",
        "            break\n",
        "        if n > target_total:\n",
        "            lo = mid   # sobe percentil -> menos picos\n",
        "        else:\n",
        "            hi = mid   # desce percentil -> mais picos\n",
        "\n",
        "    return best_pt, best_pf\n",
        "\n",
        "def select_anchor_and_targets(pt, pf):\n",
        "    if pt.size == 0 or pf.size == 0:\n",
        "        return None, []\n",
        "    idx_sorted = np.argsort(pt)\n",
        "    i = idx_sorted[len(idx_sorted)//2]  # mediana temporal\n",
        "    t_a, f_a = float(pt[i]), float(pf[i])\n",
        "    mask = (pt > t_a + FAN_T_MIN) & (pt < t_a + FAN_T_MAX)\n",
        "    cand = np.where(mask)[0]\n",
        "    cand = cand[np.argsort(pt[cand])]\n",
        "    lines = [(t_a, f_a, float(pt[j]), float(pf[j])) for j in cand[:FAN_FANOUT]]\n",
        "    return (t_a, f_a), lines\n",
        "\n",
        "def style_axes(fig, fmin, fmax, row=None, col=None, times=None):\n",
        "    ykw = (dict(type=\"log\", range=[math.log10(max(1, fmin)), math.log10(fmax)])\n",
        "           if USE_LOG_Y else dict(type=\"linear\", range=[fmin, fmax]))\n",
        "    tk = _nice_tick_params(times)\n",
        "    xkw = dict(showline=True, mirror=True, ticks=\"outside\", showgrid=True,\n",
        "               tickmode=\"linear\", dtick=tk[\"dtick\"], tick0=tk[\"tick0\"], tickformat=\".0f\")\n",
        "    fig.update_yaxes(**ykw, showgrid=True, zeroline=False, row=row, col=col)\n",
        "    fig.update_xaxes(**xkw, row=row, col=col)\n",
        "\n",
        "def add_titles(fig, main, sub):\n",
        "    fig.update_layout(template=\"plotly_white\", width=IMG_W, height=IMG_H,\n",
        "                      paper_bgcolor=\"white\", plot_bgcolor=\"white\",\n",
        "                      margin=dict(l=80, r=30, t=95, b=55))\n",
        "    fig.add_annotation(text=main, xref=\"paper\", yref=\"paper\", x=0, y=1.15,\n",
        "                       showarrow=False, font=dict(size=22, color=\"#1f2a44\"))\n",
        "    fig.add_annotation(text=sub, xref=\"paper\", yref=\"paper\", x=0, y=1.08,\n",
        "                       showarrow=False, font=dict(size=14, color=\"#324a66\"))\n",
        "\n",
        "# ================== PLOT DE 1 BLOCO ==================\n",
        "def plot_block(row, out_dir, dual=True, overlay=True):\n",
        "    # blobs\n",
        "    S_blk  = bytes_to_ndarray(row.get(\"stft_bytes\"))\n",
        "    freqs  = bytes_to_ndarray(row.get(\"freqs_bytes\"))\n",
        "    times  = bytes_to_ndarray(row.get(\"times_bytes\"))\n",
        "    if S_blk.size == 0: raise ValueError(\"stft_bytes vazio\")\n",
        "\n",
        "    # corre√ß√µes e recorte Y\n",
        "    S_db, freqs, times, zlim = detect_scale_and_fix(S_blk, freqs, times)\n",
        "    fmin, fmax = YLIM_HZ\n",
        "    freqs = np.maximum(freqs, 0.0)\n",
        "    band = (freqs >= fmin) & (freqs <= fmax)\n",
        "    S_show, F_show = S_db[band, :], freqs[band]\n",
        "\n",
        "    # picos\n",
        "    if AUTO_PEAK_DENSITY:\n",
        "        pt, pf = estimate_peaks_auto_density(S_db, freqs, times)\n",
        "    else:\n",
        "        pt, pf = _peaks_with_percentile(S_db, freqs, times, pctl=95.0)\n",
        "    if pf.size:\n",
        "        sel = (pf >= fmin) & (pf <= fmax)\n",
        "        pf, pt = pf[sel], pt[sel]\n",
        "\n",
        "    # metadados (copiados do parquet)\n",
        "    title = row.get(\"title\", \"Desconhecida\")\n",
        "    val, aro = row.get(\"valence\", None), row.get(\"arousal\", None)\n",
        "    emo = row.get(\"emotion_label\", None)\n",
        "    nfft = int(row.get(\"stft_n_fft\", 2048) or 2048)\n",
        "    hop  = int(row.get(\"stft_hop\",   512)  or 512)\n",
        "    win  = str(row.get(\"stft_window\", \"hann\"))\n",
        "\n",
        "    main = f\"{TITLE_PREFIX}{title}\"\n",
        "    sub  = f\"Bloco {int(row['block_idx'])} ‚Äî STFT {win} {nfft}/{hop}\"\n",
        "    if pd.notna(val) and pd.notna(aro): sub += f\" ‚Äî Val√™ncia {val:.3f}, Arousal {aro:.3f}\"\n",
        "    if isinstance(emo, str) and emo:    sub += f\" ‚Äî R√≥tulo: {emo}\"\n",
        "\n",
        "    blk = int(row[\"block_idx\"])\n",
        "    base = os.path.join(out_dir, f\"block_{blk:04d}\")\n",
        "    ensure_dir(out_dir)\n",
        "\n",
        "    anchor_info, lines = select_anchor_and_targets(pt, pf)\n",
        "\n",
        "    # Dual (sem c√≠rculo; mant√©m fan-lines se habilitadas)\n",
        "    if dual:\n",
        "        fig = make_subplots(rows=1, cols=2, column_widths=[0.6, 0.4], horizontal_spacing=0.08)\n",
        "        fig.add_trace(go.Heatmap(z=S_show, x=times, y=F_show, colorscale=SPEC_CMAP,\n",
        "                                 zmin=zlim[0], zmax=zlim[1],\n",
        "                                 colorbar=dict(title=\"Magnitude (dB)\", thickness=14, x=1.02)),\n",
        "                      row=1, col=1)\n",
        "        fig.add_trace(go.Scatter(x=pt, y=pf, mode=\"markers\", marker=PK_MARKER, name=\"Picos\"),\n",
        "                      row=1, col=2)\n",
        "        if anchor_info and SHOW_FAN_LINES:\n",
        "            for (x0, y0, x1, y1) in lines:\n",
        "                fig.add_shape(type=\"line\", x0=x0, y0=y0, x1=x1, y1=y1,\n",
        "                              line=dict(width=1, color=\"rgba(0,0,0,0.45)\"), row=1, col=2)\n",
        "        style_axes(fig, fmin, fmax, row=1, col=1, times=times)\n",
        "        style_axes(fig, fmin, fmax, row=1, col=2, times=times)\n",
        "        add_titles(fig, main, sub)\n",
        "        if EXPORT_HTML:\n",
        "            fig.write_html(base + \"_dual.html\", include_plotlyjs=\"cdn\")\n",
        "\n",
        "    # Overlay (sem c√≠rculo; mant√©m fan-lines se habilitadas)\n",
        "    if overlay:\n",
        "        fig2 = make_subplots(rows=1, cols=1)\n",
        "        fig2.add_trace(go.Heatmap(z=S_show, x=times, y=F_show, colorscale=SPEC_CMAP,\n",
        "                                  zmin=zlim[0], zmax=zlim[1],\n",
        "                                  colorbar=dict(title=\"Magnitude (dB)\", thickness=14, x=1.02)))\n",
        "        fig2.add_trace(go.Scatter(x=pt, y=pf, mode=\"markers\", marker=PK_MARKER, name=\"Picos\"))\n",
        "        if anchor_info and SHOW_FAN_LINES:\n",
        "            for (x0, y0, x1, y1) in lines:\n",
        "                fig2.add_shape(type=\"line\", x0=x0, y0=y0, x1=x1, y1=y1,\n",
        "                               line=dict(width=1, color=\"rgba(0,0,0,0.45)\"))\n",
        "        style_axes(fig2, fmin, fmax, row=1, col=1, times=times)\n",
        "        add_titles(fig2, main, sub + \" ‚Äî Overlay\")\n",
        "        if EXPORT_HTML:\n",
        "            fig2.write_html(base + \"_overlay.html\", include_plotlyjs=\"cdn\")\n",
        "\n",
        "    # DF de picos (inclui VA/label do bloco p/ valida√ß√£o)\n",
        "    n = len(pt)\n",
        "    return pd.DataFrame({\n",
        "        \"song_id\":       [row.get(\"song_id\")] * n,\n",
        "        \"block_idx\":     [blk] * n,\n",
        "        \"time_s\":        pt.astype(float) if n else np.array([], dtype=float),\n",
        "        \"freq_hz\":       pf.astype(float) if n else np.array([], dtype=float),\n",
        "        \"title\":         [title] * n,\n",
        "        \"valence\":       [val] * n,\n",
        "        \"arousal\":       [aro] * n,\n",
        "        \"emotion_label\": [emo] * n\n",
        "    })\n",
        "\n",
        "# ================ UTIL DE I/O ================\n",
        "def _iter_song_parquets(in_path):\n",
        "    p = pathlib.Path(in_path)\n",
        "    if p.is_dir():\n",
        "        files = sorted(p.glob(\"song_*.parquet\"))\n",
        "    else:\n",
        "        files = [p]\n",
        "    out = []\n",
        "    for fp in files:\n",
        "        m = re.search(r\"song_(\\d+)\\.parquet$\", fp.name)\n",
        "        sid = m.group(1) if m else fp.stem\n",
        "        out.append((str(fp), sid))\n",
        "    return out\n",
        "\n",
        "# ================ LOOP PRINCIPAL ================\n",
        "def process_all_songs():\n",
        "    songs = _iter_song_parquets(IN_PATH)\n",
        "    if not songs:\n",
        "        print(f\"[ERRO] Nenhum parquet encontrado em: {IN_PATH}\")\n",
        "        return\n",
        "\n",
        "    song_iter = tqdm(songs, desc=\"Songs\", unit=\"song\") if tqdm else songs\n",
        "    all_index_rows = []\n",
        "\n",
        "    for song_path, song_id_str in song_iter:\n",
        "        try:\n",
        "            df = pd.read_parquet(song_path)\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[ERRO] Song {song_id_str}: falha ao ler parquet: {e}\")\n",
        "            continue\n",
        "\n",
        "        # normaliza/ordena\n",
        "        if \"block_idx\" in df.columns:\n",
        "            df[\"block_idx\"] = pd.to_numeric(df[\"block_idx\"], errors=\"coerce\").astype(\"Int64\")\n",
        "            df = df.sort_values(\"block_idx\").reset_index(drop=True)\n",
        "\n",
        "        # ---- NOVO: ignorar blocos sem valence/arousal\n",
        "        if {\"valence\",\"arousal\"}.issubset(df.columns):\n",
        "            df = df[df[\"valence\"].notna() & df[\"arousal\"].notna()].copy()\n",
        "        else:\n",
        "            print(f\"[WARN] Song {song_id_str}: colunas valence/arousal ausentes ‚Äî pulando m√∫sica.\")\n",
        "            continue\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"[WARN] Song {song_id_str}: todos os blocos sem VA ‚Äî pulando m√∫sica.\")\n",
        "            continue\n",
        "\n",
        "        # pasta por m√∫sica\n",
        "        try:\n",
        "            song_id  = int(df.loc[df[\"block_idx\"].idxmin(), \"song_id\"]) if \"song_id\" in df else int(song_id_str)\n",
        "        except Exception:\n",
        "            song_id = int(song_id_str) if str(song_id_str).isdigit() else 0\n",
        "        song_tit = str(df.loc[df[\"block_idx\"].idxmin(), \"title\"]) if \"title\" in df else \"Desconhecida\"\n",
        "        song_dir = os.path.join(OUT_ROOT, f\"{song_id:04d}_{slugify(song_tit)}\")\n",
        "        ensure_dir(song_dir)\n",
        "\n",
        "        # progresso por blocos\n",
        "        blocks = list(df.itertuples(index=False))\n",
        "        blk_iter = tqdm(blocks, desc=f\"Song {song_id_str}\", unit=\"blk\", leave=False) if tqdm else blocks\n",
        "\n",
        "        peaks_parts = []\n",
        "        for row in blk_iter:\n",
        "            row = row._asdict() if hasattr(row, \"_asdict\") else row\n",
        "            try:\n",
        "                dfp = plot_block(row, song_dir, dual=DO_DUAL, overlay=DO_OVERLAY)\n",
        "                peaks_parts.append(dfp)\n",
        "            except Exception as e:\n",
        "                print(f\"\\n[ERRO] Song {song_id_str} bloco {row.get('block_idx')}: {e}\")\n",
        "\n",
        "        # salvar parquet de picos da m√∫sica\n",
        "        if peaks_parts:\n",
        "            peaks_df = pd.concat(peaks_parts, ignore_index=True)\n",
        "            out_pq   = os.path.join(song_dir, \"peaks_all_blocks.parquet\")\n",
        "            peaks_df.to_parquet(out_pq, index=False, compression=\"snappy\")\n",
        "            all_index_rows.append(dict(song_id=song_id, title=song_tit, peaks_path=out_pq,\n",
        "                                       n_points=len(peaks_df)))\n",
        "        else:\n",
        "            print(f\"[WARN] Song {song_id_str}: nenhum pico coletado.\")\n",
        "\n",
        "    # √≠ndice geral (opcional)\n",
        "    if all_index_rows:\n",
        "        idx_df = pd.DataFrame(all_index_rows).sort_values(\"song_id\")\n",
        "        ensure_dir(OUT_ROOT)\n",
        "        idx_path = os.path.join(OUT_ROOT, \"peaks_index.parquet\")\n",
        "        idx_df.to_parquet(idx_path, index=False, compression=\"snappy\")\n",
        "        print(f\"\\n[OK] √çndice de picos salvo em: {idx_path}\")\n",
        "    print(\"[DONE] Lote conclu√≠do.\")\n",
        "\n",
        "# ---- run ----\n",
        "process_all_songs()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNMiTS-Ss4Ca"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ===============================================================\n",
        "#  EMOTION FINGERPRINT INDEXED (TCC ‚Äî busca por emo√ß√µes)\n",
        "#  + LOGS DETALHADOS DE EXECU√á√ÉO\n",
        "# ===============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, math, warnings, time, re, unicodedata   # <-- add re, unicodedata\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa as lb\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except Exception:\n",
        "    tqdm = None\n",
        "\n",
        "# ===============================================================\n",
        "# CONFIGURA√á√ïES\n",
        "# ===============================================================\n",
        "\n",
        "PEAKS_INDEX_PARQUET = \"/content/drive/MyDrive/DataSet TCC/DEAM/fingerprint/peaks_index.parquet\"\n",
        "\n",
        "@dataclass\n",
        "class FPConfig:\n",
        "    n_fft: int = 2048\n",
        "    hop_length: int = 512\n",
        "    window: str = \"hann\"\n",
        "    center: bool = True\n",
        "    to_db: bool = True\n",
        "    topk_per_col: int = 5\n",
        "    fan_out: int = 5\n",
        "    min_dt: float = 0.02\n",
        "    max_dt: float = 3.0\n",
        "    min_df: float = 50.0\n",
        "    sr: int = 22050\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# STFT PADRONIZADA (para fingerprint)\n",
        "# ===============================================================\n",
        "def compute_stft(y: np.ndarray, sr: int, n_fft: int = 2048, hop_length: int = 512,\n",
        "                 window: str = \"hann\", center: bool = True, to_db: bool = True) -> Dict[str, Any]:\n",
        "    S = lb.stft(y, n_fft=n_fft, hop_length=hop_length, window=window, center=center)\n",
        "    mag = np.abs(S)\n",
        "    if to_db:\n",
        "        mag = lb.amplitude_to_db(mag, ref=np.max)\n",
        "    times = lb.frames_to_time(np.arange(mag.shape[1]), sr=sr, hop_length=hop_length, n_fft=n_fft)\n",
        "    freqs = lb.fft_frequencies(sr=sr, n_fft=n_fft)\n",
        "    meta = dict(sr=sr, n_fft=n_fft, hop_length=hop_length, window=window, center=center, scale_db=to_db)\n",
        "    return {\"spec\": mag, \"times\": times, \"freqs\": freqs, \"meta\": meta}\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# CARREGAMENTO E FILTRAGEM\n",
        "# ===============================================================\n",
        "def load_index(path=PEAKS_INDEX_PARQUET) -> pd.DataFrame:\n",
        "    print(f\"\\nüìÇ Carregando √≠ndice global de picos: {path}\")\n",
        "    df = pd.read_parquet(path)\n",
        "    expected = [\"song_id\", \"title\", \"peaks_path\", \"n_points\"]\n",
        "    missing = [c for c in expected if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Colunas ausentes no √≠ndice: {missing}\")\n",
        "    df[\"song_id\"] = df[\"song_id\"].astype(\"Int32\")\n",
        "    df[\"title\"] = df[\"title\"].astype(str)\n",
        "    df[\"peaks_path\"] = df[\"peaks_path\"].astype(str)\n",
        "    df[\"n_points\"] = pd.to_numeric(df[\"n_points\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    print(f\"‚úÖ √çndice carregado com {len(df)} m√∫sicas.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_peaks_from_path(peaks_path: str, columns: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "    if not os.path.exists(peaks_path):\n",
        "        raise FileNotFoundError(f\"Arquivo de picos n√£o encontrado: {peaks_path}\")\n",
        "    return pd.read_parquet(peaks_path, columns=columns)\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# FINGERPRINT (Shazam-like)\n",
        "# ===============================================================\n",
        "def detect_peaks_constellation(S_db: np.ndarray, cfg: FPConfig) -> List[Tuple[int, int]]:\n",
        "    H, W = S_db.shape\n",
        "    peaks = []\n",
        "    for x in range(W):\n",
        "        col = S_db[:, x]\n",
        "        idx = np.argpartition(col, -cfg.topk_per_col)[-cfg.topk_per_col:]\n",
        "        idx = idx[np.argsort(col[idx])][::-1]\n",
        "        for fbin in idx:\n",
        "            peaks.append((x, fbin))\n",
        "    return peaks\n",
        "\n",
        "\n",
        "def pairs_to_hashes(peaks: List[Tuple[int, int]], cfg: FPConfig, sr: int) -> List[Tuple[int, float, int, float, int]]:\n",
        "    hashes = []\n",
        "    hop_t = cfg.hop_length / sr\n",
        "    hz_per_bin = (sr / cfg.n_fft) / 2.0\n",
        "    for i, (xa, fa) in enumerate(peaks):\n",
        "        for j in range(1, cfg.fan_out + 1):\n",
        "            if i + j >= len(peaks): break\n",
        "            xt, ft = peaks[i + j]\n",
        "            dt = (xt - xa) * hop_t\n",
        "            df = (ft - fa) * hz_per_bin\n",
        "            if dt < cfg.min_dt or dt > cfg.max_dt: continue\n",
        "            if abs(df) < cfg.min_df: continue\n",
        "            q_dt = int(round(dt * 10))\n",
        "            q_df = int(round(df / 20.0))\n",
        "            h = (q_df & 0xFFFF) << 16 | (q_dt & 0xFFFF)\n",
        "            hashes.append((h, xa * hop_t, fa, xt * hop_t, ft))\n",
        "    return hashes\n",
        "\n",
        "\n",
        "def fingerprint_wav(path: str, cfg=FPConfig()) -> pd.DataFrame:\n",
        "    print(f\"\\nüéß Gerando fingerprint do clipe: {os.path.basename(path)}\")\n",
        "    y, sr = lb.load(path, sr=cfg.sr, mono=True)\n",
        "    print(f\"   - Dura√ß√£o: {len(y)/sr:.2f}s  SR={sr}\")\n",
        "    stft_out = compute_stft(y, sr, n_fft=cfg.n_fft, hop_length=cfg.hop_length,\n",
        "                            window=cfg.window, center=cfg.center, to_db=cfg.to_db)\n",
        "    S_db = stft_out[\"spec\"]\n",
        "    peaks = detect_peaks_constellation(S_db, cfg)\n",
        "    hashes = pairs_to_hashes(peaks, cfg, sr)\n",
        "    print(f\"   - {len(peaks)} picos detectados ‚Üí {len(hashes)} hashes gerados\")\n",
        "    return pd.DataFrame(hashes, columns=[\"hash\", \"tq\", \"fq\", \"tq2\", \"fq2\"])\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# BUSCA (com LOGS) ‚Äî AGORA COM EMO√á√ïES COMPOSTAS/SIN√îNIMOS\n",
        "# ===============================================================\n",
        "def _normalize_text(text: str) -> str:\n",
        "    \"\"\"remove acentos e coloca em min√∫sculas.\"\"\"\n",
        "    text = unicodedata.normalize(\"NFKD\", str(text))\n",
        "    text = \"\".join(c for c in text if not unicodedata.combining(c))\n",
        "    return text.lower().strip()\n",
        "\n",
        "# mapeamento opcional de sin√¥nimos ‚Üí r√≥tulo can√¥nico (ajuste ao seu √≠ndice)\n",
        "EMOTION_SYNONYMS = {\n",
        "    # quadrante excitado/positivo\n",
        "    \"excitante\": \"excitado_positivo\",\n",
        "    \"positivo\":  \"excitado_positivo\",\n",
        "    \"feliz\":     \"excitado_positivo\",\n",
        "    \"euforico\":  \"excitado_positivo\",\n",
        "    \"energetico\":\"excitado_positivo\",\n",
        "    \"animado\":   \"excitado_positivo\",\n",
        "    \"alegre\":    \"excitado_positivo\",\n",
        "    # voc√™ pode expandir com outros quadrantes aqui...\n",
        "}\n",
        "\n",
        "def _expand_emotion_keywords(emotion: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Divide a emo√ß√£o composta e normaliza:\n",
        "    'Excitante/positivo (feliz, euf√≥rico, en√©rgico)'\n",
        "      ‚Üí ['excitante','positivo','feliz','euforico','energetico']\n",
        "    \"\"\"\n",
        "    base = _normalize_text(emotion)\n",
        "    parts = re.split(r\"[/,;()]+\", base)\n",
        "    parts = [p.strip() for p in parts if p.strip()]\n",
        "    # substitui por can√¥nicos quando houver\n",
        "    out = []\n",
        "    for p in parts:\n",
        "        out.append(EMOTION_SYNONYMS.get(p, p))\n",
        "    # remove duplicatas preservando ordem\n",
        "    seen, uniq = set(), []\n",
        "    for p in out:\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            uniq.append(p)\n",
        "    return uniq\n",
        "\n",
        "def search_clip(peaks_index: pd.DataFrame,\n",
        "                emotion: Optional[str] = None,\n",
        "                prefilter_top: int = 50,\n",
        "                results_topn: int = 10) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Busca emocional no √≠ndice de fingerprints (sem clipe externo).\n",
        "    Agora aceita emo√ß√µes compostas/sin√¥nimos.\n",
        "    \"\"\"\n",
        "    print(\"\\nüîç Iniciando busca no √≠ndice de fingerprints...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Fase 0 ‚Äî Filtragem emocional (composta/sin√¥nimos)\n",
        "    if emotion and \"emotion_zone\" in peaks_index.columns:\n",
        "        keys = _expand_emotion_keywords(emotion)\n",
        "        print(f\"üé≠ Emo√ß√£o solicitada : {emotion}\")\n",
        "        print(f\"üîπ Palavras/labels  : {keys}\")\n",
        "        col_norm = peaks_index[\"emotion_zone\"].astype(str).apply(_normalize_text)\n",
        "\n",
        "        # monta m√°scara: casa se QUALQUER termo aparecer\n",
        "        mask = pd.Series(False, index=peaks_index.index)\n",
        "        for kw in keys:\n",
        "            # procura por palavra inteira OU r√≥tulo can√¥nico\n",
        "            mask |= col_norm.str.contains(rf\"\\b{re.escape(kw)}\\b\", regex=True)\n",
        "        df = peaks_index.loc[mask].copy()\n",
        "        print(f\"üéØ {len(df)} m√∫sicas encontradas com pelo menos um termo.\")\n",
        "    else:\n",
        "        df = peaks_index.copy()\n",
        "        print(f\"üé≠ Nenhum filtro emocional aplicado ({len(df)} m√∫sicas no total).\")\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"‚ùå Nenhum resultado encontrado no √≠ndice.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Fase 1 ‚Äî Ordena√ß√£o preliminar (densidade de picos)\n",
        "    df = df.sort_values(\"n_points\", ascending=False).head(prefilter_top)\n",
        "    print(f\"üìä Selecionando Top-{prefilter_top} por n√∫mero de picos (densidade espectral).\")\n",
        "\n",
        "    # Fase 2 ‚Äî Estat√≠stica simples\n",
        "    avg_points = df[\"n_points\"].mean()\n",
        "    print(f\"üìà M√©dia de picos nas m√∫sicas selecionadas: {avg_points:.1f}\")\n",
        "\n",
        "    # Fase 3 ‚Äî Ranking final\n",
        "    df_final = df.head(results_topn).reset_index(drop=True)\n",
        "    dur = time.time() - start_time\n",
        "    print(f\"\\nüèÅ Busca conclu√≠da em {dur:.2f}s ‚Äî {len(df_final)} melhores resultados:\\n\")\n",
        "    cols_show = [\"song_id\", \"title\", \"n_points\", \"emotion_zone\"] if \"emotion_zone\" in df_final.columns else [\"song_id\",\"title\",\"n_points\"]\n",
        "    print(df_final[cols_show])\n",
        "\n",
        "    return df_final\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# PLOT PARA INSERIR NO TCC\n",
        "# ===============================================================\n",
        "def plot_constellation(peaks_path: str, title: str = \"\", max_points=2000):\n",
        "    import plotly.graph_objects as go\n",
        "    df = load_peaks_from_path(peaks_path)\n",
        "    if \"t_anchor\" not in df.columns or \"f_anchor\" not in df.columns:\n",
        "        raise ValueError(\"Arquivo sem colunas t_anchor/f_anchor\")\n",
        "    if len(df) > max_points:\n",
        "        df = df.sample(max_points, random_state=0)\n",
        "    fig = go.Figure(go.Scattergl(x=df[\"t_anchor\"], y=df[\"f_anchor\"],\n",
        "                                 mode=\"markers\", marker=dict(size=3)))\n",
        "    fig.update_layout(template=\"plotly_white\", width=900, height=420,\n",
        "                      xaxis_title=\"Tempo (s)\", yaxis_title=\"Frequ√™ncia (Hz)\",\n",
        "                      title=f\"Constela√ß√£o ‚Äî {title or os.path.basename(peaks_path)}\")\n",
        "    return fig\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# EXEMPLO DE EXECU√á√ÉO\n",
        "# ===============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    index_df = load_index(PEAKS_INDEX_PARQUET)\n",
        "    print(f\"\\nüî¢ Total de m√∫sicas no √≠ndice: {len(index_df)}\")\n",
        "\n",
        "    # Exemplo 1 ‚Äî emo√ß√£o composta com sin√¥nimos:\n",
        "    results = search_clip(\n",
        "        index_df,\n",
        "        emotion=\"Excitante/positivo (feliz, euf√≥rico, en√©rgico)\",\n",
        "        results_topn=10\n",
        "    )\n",
        "\n",
        "    # Exemplo 2 ‚Äî sem filtro emocional:\n",
        "    # results = search_clip(index_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w952mrQ2mK9"
      },
      "source": [
        "## Teste 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UF7HiKU3DC7"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHO7w6FL2pIe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, hashlib\n",
        "\n",
        "IN_BLOCKS_DIR  = Path(\"/content/drive/MyDrive/DataSet TCC/DEAM/parquet/blocks_by_song\")\n",
        "OUT_FINGER_DIR = Path(\"/content/drive/MyDrive/DataSet TCC/DEAM/fingerprint\")\n",
        "\n",
        "OUT_FINGER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PEAKS_INDEX_PATH = OUT_FINGER_DIR / \"peaks_index.parquet\"\n",
        "REPORTS_DIR      = OUT_FINGER_DIR / \"_reports\"\n",
        "REPORTS_DIR.mkdir(exist_ok=True, parents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgKXwrOM3ODn"
      },
      "source": [
        "### VA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9uMRu6I3PLn"
      },
      "outputs": [],
      "source": [
        "# Quadrantes (Russell): Eixo X = Valence, Eixo Y = Arousal\n",
        "QUADRANT_NAMES = {\n",
        "    \"Q1\": \"Feliz/Excitado (V+ A+)\",   # high valence, high arousal\n",
        "    \"Q2\": \"Tenso/Ansioso (V- A+)\",    # low valence, high arousal\n",
        "    \"Q3\": \"Triste/Let√°rgico (V- A-)\", # low valence, low arousal\n",
        "    \"Q4\": \"Calmo/Relaxado (V+ A-)\",   # high valence, low arousal\n",
        "}\n",
        "\n",
        "# Limiares de z-score ou escala normalizada (ajuste se usa VA z-score ou VA [0,1])\n",
        "# Aqui assumo VA centrado em 0; mude para seus thresholds reais.\n",
        "VALENCE_TH = 0.0\n",
        "AROUSAL_TH = 0.0\n",
        "\n",
        "def va_to_quadrant(valence: float, arousal: float) -> str:\n",
        "    if pd.isna(valence) or pd.isna(arousal):\n",
        "        return \"NA\"\n",
        "    if valence >= VALENCE_TH and arousal >= AROUSAL_TH:\n",
        "        return \"Q1\"\n",
        "    if valence < VALENCE_TH and arousal >= AROUSAL_TH:\n",
        "        return \"Q2\"\n",
        "    if valence < VALENCE_TH and arousal < AROUSAL_TH:\n",
        "        return \"Q3\"\n",
        "    return \"Q4\"\n",
        "\n",
        "# ----- DICION√ÅRIO DE SIN√îNIMOS (PT/EN) POR QUADRANTE -----\n",
        "EMO_SYNONYMS = {\n",
        "    \"Q1\": [\n",
        "        \"feliz\",\"euf√≥rico\",\"animado\",\"energ√©tico\",\"empolgado\",\"alegre\",\n",
        "        \"happy\",\"excited\",\"joyful\",\"energetic\",\"upbeat\",\"elated\"\n",
        "    ],\n",
        "    \"Q2\": [\n",
        "        \"tenso\",\"ansioso\",\"nervoso\",\"raiva\",\"irritado\",\"intenso\",\n",
        "        \"tense\",\"anxious\",\"angry\",\"irritated\",\"agitated\",\"stressed\"\n",
        "    ],\n",
        "    \"Q3\": [\n",
        "        \"triste\",\"depressivo\",\"melanc√≥lico\",\"abatido\",\"let√°rgico\",\n",
        "        \"sad\",\"depressed\",\"melancholic\",\"blue\",\"down\",\"lethargic\"\n",
        "    ],\n",
        "    \"Q4\": [\n",
        "        \"calmo\",\"relaxado\",\"tranquilo\",\"sereno\",\"confort√°vel\",\"suave\",\n",
        "        \"calm\",\"relaxed\",\"peaceful\",\"soothing\",\"chill\",\"serene\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "def expand_emotion_query(q: str) -> set:\n",
        "    \"\"\"Recebe um termo (ex.: 'feliz') e devolve o conjunto de termos equivalentes nos 4 quadrantes.\"\"\"\n",
        "    qnorm = (q or \"\").strip().lower()\n",
        "    hits = set()\n",
        "    for quad, words in EMO_SYNONYMS.items():\n",
        "        for w in words:\n",
        "            if qnorm in w or w in qnorm:\n",
        "                hits.update(words)\n",
        "    # Se n√£o bateu sin√¥nimo algum, considere que o usu√°rio quer string literal:\n",
        "    if not hits and qnorm:\n",
        "        hits.add(qnorm)\n",
        "    return hits\n",
        "\n",
        "def mask_by_emotion_zone(df: pd.DataFrame, query: str) -> pd.Series:\n",
        "    \"\"\"Filtra por emotion_zone (texto) usando sin√¥nimos expans√≠veis.\"\"\"\n",
        "    if \"emotion_zone\" not in df.columns:\n",
        "        return pd.Series([True] * len(df), index=df.index)  # sem coluna, n√£o filtra\n",
        "    synonyms = expand_emotion_query(query)\n",
        "    if not synonyms:\n",
        "        return pd.Series([True] * len(df), index=df.index)\n",
        "    s = df[\"emotion_zone\"].fillna(\"\").str.lower()\n",
        "    mask = s.apply(lambda x: any(term in x for term in synonyms))\n",
        "    return mask\n",
        "\n",
        "def plot_va_quadrants(index_df: pd.DataFrame, title=\"Mapa Valence‚ÄìArousal (Exemplos)\"):\n",
        "    \"\"\"\n",
        "    Plota VA do √≠ndice global, colorindo por quadrante.\n",
        "    Espera colunas: 'valence', 'arousal' no index_df (p.ex., m√©dias por m√∫sica).\n",
        "    \"\"\"\n",
        "    df = index_df.copy()\n",
        "    if \"valence\" not in df.columns or \"arousal\" not in df.columns:\n",
        "        raise ValueError(\"index_df precisa ter colunas 'valence' e 'arousal'.\")\n",
        "\n",
        "    df[\"quadrant\"] = [va_to_quadrant(v, a) for v, a in zip(df[\"valence\"], df[\"arousal\"])]\n",
        "\n",
        "    color_map = {\"Q1\":\"#2ca02c\",\"Q2\":\"#d62728\",\"Q3\":\"#1f77b4\",\"Q4\":\"#ff7f0e\",\"NA\":\"#7f7f7f\"}\n",
        "    fig = go.Figure()\n",
        "    for qk, qname in QUADRANT_NAMES.items():\n",
        "        sub = df[df[\"quadrant\"] == qk]\n",
        "        if len(sub)==0:\n",
        "            continue\n",
        "        fig.add_scatter(\n",
        "            x=sub[\"valence\"], y=sub[\"arousal\"], mode=\"markers\",\n",
        "            name=qname, marker=dict(size=8, opacity=0.75),\n",
        "            marker_color=color_map.get(qk, \"#7f7f7f\"),\n",
        "            text=sub.get(\"title\", \"\")\n",
        "        )\n",
        "\n",
        "    # Eixos e cruz central (0,0)\n",
        "    fig.update_layout(\n",
        "        title=title, xaxis_title=\"Valence\", yaxis_title=\"Arousal\",\n",
        "        width=800, height=600, template=\"plotly_white\"\n",
        "    )\n",
        "    fig.add_hline(y=AROUSAL_TH, line_width=1, line_dash=\"dot\", line_color=\"#444\")\n",
        "    fig.add_vline(x=VALENCE_TH, line_width=1, line_dash=\"dot\", line_color=\"#444\")\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMLDiSBq3P8o"
      },
      "source": [
        "### Fun√ß√µes de hashing e salvamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmZpU6CD3Sbo"
      },
      "outputs": [],
      "source": [
        "# --- EXEMPLO de quantiza√ß√£o usada no seu pipeline (ajuste se precisar) ---\n",
        "def _q(v: float, step: float) -> int:\n",
        "    return int(np.round(v / step))\n",
        "\n",
        "def make_pair_hash(f_anchor_hz: float, f_target_hz: float,\n",
        "                   dt_sec: float, df_hz: float,\n",
        "                   q_dt: float, q_df: float, q_f: float) -> str:\n",
        "    \"\"\"\n",
        "    Gera um hash curto e est√°vel a partir dos valores quantizados.\n",
        "    \"\"\"\n",
        "    fa_q  = _q(f_anchor_hz, q_f)\n",
        "    ft_q  = _q(f_target_hz, q_f)\n",
        "    dt_q  = _q(dt_sec,      q_dt)\n",
        "    df_q  = _q(df_hz,       q_df)\n",
        "\n",
        "    # Canonical tuple ‚Üí bytes ‚Üí blake2 (r√°pido, est√°vel)\n",
        "    payload = f\"{fa_q}|{ft_q}|{dt_q}|{df_q}\".encode(\"utf-8\")\n",
        "    h = hashlib.blake2b(payload, digest_size=8).hexdigest()  # 16 hex chars\n",
        "    return h\n",
        "\n",
        "def attach_uniqueness_keys(peaks_pairs_df: pd.DataFrame,\n",
        "                           song_id, block_idx) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Garante chaves expl√≠citas e verific√°veis por linha de par √¢ncora‚Äìalvo.\n",
        "    Sup√µe colunas: t_anchor, f_anchor, t_target, f_target, hash.\n",
        "    \"\"\"\n",
        "    df = peaks_pairs_df.copy()\n",
        "    df[\"song_id\"]   = song_id\n",
        "    df[\"block_idx\"] = block_idx\n",
        "\n",
        "    # UID expl√≠cido (n√£o substitui 'hash'); ajuda a depurar colis√µes entre blocos\n",
        "    uid_str = (df[\"song_id\"].astype(str) + \"|\" +\n",
        "               df[\"block_idx\"].astype(str) + \"|\" +\n",
        "               df[\"t_anchor\"].round(4).astype(str) + \"|\" +\n",
        "               df[\"f_anchor\"].round(1).astype(str) + \"|\" +\n",
        "               df[\"t_target\"].round(4).astype(str) + \"|\" +\n",
        "               df[\"f_target\"].round(1).astype(str) + \"|\" +\n",
        "               df[\"hash\"].astype(str))\n",
        "    df[\"uid\"] = uid_str.apply(lambda s: hashlib.blake2b(s.encode(\"utf-8\"), digest_size=10).hexdigest())\n",
        "    return df\n",
        "\n",
        "def persist_peaks_with_stft_meta(peaks_df: pd.DataFrame,\n",
        "                                 out_parquet_path,\n",
        "                                 stft_meta: dict):\n",
        "    \"\"\"\n",
        "    Persiste picos + METADADOS STFT como colunas (stft_nfft, stft_hop, stft_window, sr, etc.).\n",
        "    \"\"\"\n",
        "    df = peaks_df.copy()\n",
        "    # Metadados que queremos materializar como colunas\n",
        "    for k, v in stft_meta.items():\n",
        "        df[k] = v\n",
        "    # Ex.: tamb√©m persistir limites gr√°ficos/peaks config:\n",
        "    defaults = dict(\n",
        "        spec_ylim_low=50, spec_ylim_high=8000,\n",
        "        zlim_low=-75, zlim_high=0,\n",
        "        peak_topk=stft_meta.get(\"peak_topk\", None),\n",
        "        peak_target_per_sec=stft_meta.get(\"target_peaks_per_sec\", None),\n",
        "        fanout=stft_meta.get(\"fan_out\", None),\n",
        "        dt_min=stft_meta.get(\"dt_min\", None),\n",
        "        dt_max=stft_meta.get(\"dt_max\", None),\n",
        "        df_min=stft_meta.get(\"df_min\", None),\n",
        "    )\n",
        "    for k, v in defaults.items():\n",
        "        if k not in df.columns:\n",
        "            df[k] = v\n",
        "    df.to_parquet(out_parquet_path, index=False)\n",
        "    return out_parquet_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekuhFm9N3bUY"
      },
      "source": [
        "### Colis√µes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VP5x7at3a7Z"
      },
      "outputs": [],
      "source": [
        "def collision_report(peaks_parquet_path: Path, out_csv: Path):\n",
        "    \"\"\"\n",
        "    Reporta colis√µes de hash dentro de um arquivo de picos por m√∫sica,\n",
        "    considerando chaves (hash) que aparecem em m√∫ltiplos blocos.\n",
        "    \"\"\"\n",
        "    df = pd.read_parquet(peaks_parquet_path)\n",
        "    if not {\"hash\",\"block_idx\"}.issubset(df.columns):\n",
        "        raise ValueError(\"Parquet sem colunas necess√°rias: 'hash', 'block_idx'\")\n",
        "\n",
        "    # quantos blocos diferentes compartilham o mesmo hash?\n",
        "    g = df.groupby(\"hash\")[\"block_idx\"].nunique().reset_index(name=\"n_blocks\")\n",
        "    collisions = g[g[\"n_blocks\"] > 1].copy()\n",
        "\n",
        "    # Exibir amostras detalhadas:\n",
        "    details = None\n",
        "    if len(collisions):\n",
        "        details = (\n",
        "            df.merge(collisions[[\"hash\"]], on=\"hash\", how=\"inner\")\n",
        "              .sort_values([\"hash\",\"block_idx\",\"t_anchor\"])\n",
        "        )\n",
        "\n",
        "    # Salva CSV com estat√≠sticas\n",
        "    stats = {\n",
        "        \"total_pairs\": len(df),\n",
        "        \"unique_hashes\": df[\"hash\"].nunique(),\n",
        "        \"colliding_hashes\": int((g[\"n_blocks\"] > 1).sum()),\n",
        "    }\n",
        "    pd.DataFrame([stats]).to_csv(out_csv.with_name(out_csv.stem + \"_stats.csv\"), index=False)\n",
        "    collisions.to_csv(out_csv, index=False)\n",
        "    if details is not None:\n",
        "        details.to_csv(out_csv.with_name(out_csv.stem + \"_details.csv\"), index=False)\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USeDdL_E3f1g"
      },
      "source": [
        "### Testes Hash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC1dZuZd3iM_"
      },
      "outputs": [],
      "source": [
        "def augment_audio(y, sr, time_stretch=1.0, pitch_shift=0.0):\n",
        "    z = y\n",
        "    if time_stretch and time_stretch != 1.0:\n",
        "        z = librosa.effects.time_stretch(z, rate=time_stretch)\n",
        "    if pitch_shift and pitch_shift != 0.0:\n",
        "        z = librosa.effects.pitch_shift(z, sr=sr, n_steps=pitch_shift)\n",
        "    return z\n",
        "\n",
        "def fingerprint_match_rate(ref_pairs: pd.DataFrame, test_pairs: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Mede interse√ß√£o de 'hash' entre conjunto refer√™ncia e teste.\n",
        "    Simples, mas √∫til para ver sensibilidade a stretch/shift.\n",
        "    \"\"\"\n",
        "    if \"hash\" not in ref_pairs.columns or \"hash\" not in test_pairs.columns:\n",
        "        return 0.0\n",
        "    a = set(ref_pairs[\"hash\"].values.tolist())\n",
        "    b = set(test_pairs[\"hash\"].values.tolist())\n",
        "    if not a:\n",
        "        return 0.0\n",
        "    return len(a & b) / len(a)\n",
        "\n",
        "def run_robustness_grid(y, sr, fp_func, grid=None):\n",
        "    \"\"\"\n",
        "    Executa uma grade de augmentations e mede match_rate.\n",
        "    - fp_func(y, sr) deve retornar DataFrame com coluna 'hash'.\n",
        "    - grid: lista de (time_stretch, pitch_shift)\n",
        "    \"\"\"\n",
        "    if grid is None:\n",
        "        grid = [(1.0,0.0), (0.9,0.0), (1.1,0.0), (1.0,1.0), (1.0,-1.0), (0.9,1.0), (1.1,-1.0)]\n",
        "    ref_df = fp_func(y, sr)  # baseline\n",
        "    results = []\n",
        "    for ts, ps in grid:\n",
        "        y_aug = augment_audio(y, sr, time_stretch=ts, pitch_shift=ps)\n",
        "        test_df = fp_func(y_aug, sr)\n",
        "        mr = fingerprint_match_rate(ref_df, test_df)\n",
        "        results.append({\"time_stretch\": ts, \"pitch_shift\": ps, \"match_rate\": mr})\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95XpNLao3oGu"
      },
      "outputs": [],
      "source": [
        "def build_index():\n",
        "    rows = []\n",
        "    for p in OUT_FINGER_DIR.glob(\"*_*/peaks_all_blocks.parquet\"):\n",
        "        try:\n",
        "            df = pd.read_parquet(p)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Falha lendo {p}: {e}\")\n",
        "            continue\n",
        "        song_id = df[\"song_id\"].iloc[0] if \"song_id\" in df.columns else None\n",
        "        title   = df[\"title\"].iloc[0]    if \"title\"   in df.columns else None\n",
        "\n",
        "        # Agrega√ß√µes √∫teis:\n",
        "        n_pairs = len(df)\n",
        "        n_hash  = df[\"hash\"].nunique() if \"hash\" in df.columns else None\n",
        "\n",
        "        # M√©dia VA por m√∫sica (se vier por bloco, na aus√™ncia de outra tabela can√¥nica)\n",
        "        valence = df[\"valence\"].dropna().mean() if \"valence\" in df.columns else None\n",
        "        arousal = df[\"arousal\"].dropna().mean() if \"arousal\" in df.columns else None\n",
        "\n",
        "        rows.append(dict(\n",
        "            song_id=song_id, title=title, peaks_path=str(p),\n",
        "            n_pairs=n_pairs, n_hash=n_hash,\n",
        "            valence=valence, arousal=arousal\n",
        "        ))\n",
        "    idx = pd.DataFrame(rows)\n",
        "    if len(idx)==0:\n",
        "        print(\"[WARN] Nada encontrado em\", OUT_FINGER_DIR)\n",
        "        return\n",
        "    idx.sort_values([\"title\",\"song_id\"], inplace=True)\n",
        "    idx.to_parquet(PEAKS_INDEX_PATH, index=False)\n",
        "    print(\"‚úî Index salvo em:\", PEAKS_INDEX_PATH)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAtrLikA3rTN"
      },
      "outputs": [],
      "source": [
        "def validate_blocks():\n",
        "    issues = []\n",
        "    for pq in IN_BLOCKS_DIR.glob(\"song_*.parquet\"):\n",
        "        try:\n",
        "            df = pd.read_parquet(pq, columns=[\"song_id\",\"block_idx\",\"valence\",\"arousal\"])\n",
        "        except Exception as e:\n",
        "            issues.append({\"path\": str(pq), \"error\": str(e), \"issue\": \"read_error\"})\n",
        "            continue\n",
        "\n",
        "        if len(df)==0:\n",
        "            issues.append({\"path\": str(pq), \"error\": \"\", \"issue\": \"empty_file\"})\n",
        "            continue\n",
        "\n",
        "        missing_v = df[\"valence\"].isna().sum() if \"valence\" in df.columns else len(df)\n",
        "        missing_a = df[\"arousal\"].isna().sum() if \"arousal\" in df.columns else len(df)\n",
        "\n",
        "        if missing_v>0 or missing_a>0 or \"valence\" not in df.columns or \"arousal\" not in df.columns:\n",
        "            issues.append({\n",
        "                \"path\": str(pq),\n",
        "                \"error\": \"\",\n",
        "                \"issue\": \"missing_valence_arousal\",\n",
        "                \"n_rows\": len(df),\n",
        "                \"missing_valence\": int(missing_v),\n",
        "                \"missing_arousal\": int(missing_a),\n",
        "            })\n",
        "\n",
        "    rep = pd.DataFrame(issues)\n",
        "    out = REPORTS_DIR / \"validate_blocks_report.csv\"\n",
        "    rep.to_csv(out, index=False)\n",
        "    print(\"‚úî Relat√≥rio salvo em:\", out)\n",
        "    print(rep.head())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    validate_blocks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2iyirMy3uug"
      },
      "outputs": [],
      "source": [
        "def _gallery_for_song(peaks_path: Path, n_blocks=4, max_points=2000):\n",
        "    df = pd.read_parquet(peaks_path)\n",
        "    if \"block_idx\" not in df.columns:\n",
        "        return None\n",
        "\n",
        "    blocks = sorted(df[\"block_idx\"].dropna().unique().tolist())\n",
        "    if not blocks:\n",
        "        return None\n",
        "    sample_blocks = random.sample(blocks, min(n_blocks, len(blocks)))\n",
        "\n",
        "    fig = go.Figure()\n",
        "    for b in sample_blocks:\n",
        "        sub = df[df[\"block_idx\"] == b]\n",
        "        sub = sub.sample(n=min(max_points, len(sub)), random_state=42) if len(sub) > max_points else sub\n",
        "        fig.add_scattergl(\n",
        "            x=sub[\"t_anchor\"], y=sub[\"f_anchor\"], mode=\"markers\",\n",
        "            name=f\"block {b}\", marker=dict(size=3), opacity=0.7\n",
        "        )\n",
        "    fig.update_layout(template=\"plotly_white\", height=500, width=800,\n",
        "                      xaxis_title=\"Tempo (s)\", yaxis_title=\"Frequ√™ncia (Hz)\",\n",
        "                      title=f\"Constellation ‚Äî {peaks_path.parent.name}\")\n",
        "    return fig\n",
        "\n",
        "def make_quadrant_galleries(index_path: Path, out_dir: Path, n_per_quad=4):\n",
        "    idx = pd.read_parquet(index_path)\n",
        "    if not {\"valence\",\"arousal\",\"peaks_path\"}.issubset(idx.columns):\n",
        "        raise ValueError(\"index precisa ter colunas 'valence', 'arousal', 'peaks_path'.\")\n",
        "\n",
        "    idx[\"quadrant\"] = [va_to_quadrant(v, a) for v, a in zip(idx[\"valence\"], idx[\"arousal\"])]\n",
        "\n",
        "    for qk, qname in QUADRANT_NAMES.items():\n",
        "        sub = idx[idx[\"quadrant\"] == qk]\n",
        "        if len(sub)==0:\n",
        "            print(f\"[INFO] Sem itens para {qname}\")\n",
        "            continue\n",
        "        osel = sub.sample(n=min(n_per_quad, len(sub)), random_state=123)\n",
        "        for _, row in osel.iterrows():\n",
        "            fig = _gallery_for_song(Path(row[\"peaks_path\"]))\n",
        "            if fig is None:\n",
        "                continue\n",
        "            out_html = out_dir / f\"gallery_{qk}_{Path(row['peaks_path']).parent.name}.html\"\n",
        "            fig.write_html(out_html, include_plotlyjs=\"cdn\")\n",
        "            print(\"‚úî\", out_html)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from common_paths import PEAKS_INDEX_PATH\n",
        "    make_quadrant_galleries(PEAKS_INDEX_PATH, REPORTS_DIR, n_per_quad=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgwm1DLM3xJx"
      },
      "outputs": [],
      "source": [
        "# Exemplo de metadados STFT e config de peaks a persistir\n",
        "stft_meta = dict(\n",
        "    stft_nfft=2048,\n",
        "    stft_hop=512,\n",
        "    stft_window=\"hann\",\n",
        "    sr=22050,\n",
        "    peak_topk=5,                    # ou seu valor real\n",
        "    target_peaks_per_sec=300,       # se voc√™ usa densidade alvo\n",
        "    fan_out=5,\n",
        "    dt_min=0.02, dt_max=0.5,        # em segundos\n",
        "    df_min=40.0,                    # em Hz\n",
        "    q_dt=0.01, q_df=20.0, q_f=20.0  # passos de quantiza√ß√£o usados no make_pair_hash\n",
        ")\n",
        "\n",
        "# Depois de gerar os pares (com colunas t_anchor, f_anchor, t_target, f_target):\n",
        "pairs[\"hash\"] = [\n",
        "    make_pair_hash(fa, ft, (tt-ta), (ft-fa), stft_meta[\"q_dt\"], stft_meta[\"q_df\"], stft_meta[\"q_f\"])\n",
        "    for fa, ft, ta, tt in zip(pairs[\"f_anchor\"], pairs[\"f_target\"], pairs[\"t_anchor\"], pairs[\"t_target\"])\n",
        "]\n",
        "\n",
        "pairs = attach_uniqueness_keys(pairs, song_id=row[\"song_id\"], block_idx=row[\"block_idx\"])\n",
        "\n",
        "# Adicione colunas de valence/arousal do bloco (se existirem no DF do bloco original)\n",
        "for col in (\"valence\", \"arousal\", \"emotion_zone\", \"title\"):\n",
        "    if col in row and col not in pairs.columns:\n",
        "        pairs[col] = row[col]\n",
        "\n",
        "persist_peaks_with_stft_meta(pairs, out_parquet_path, stft_meta)\n",
        "\n",
        "# Opcional: ap√≥s salvar cada m√∫sica, emita relat√≥rio de colis√µes\n",
        "stats = collision_report(out_parquet_path, out_parquet_path.with_name(\"hash_collisions.csv\"))\n",
        "print(\"Collision stats:\", stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from pathlib import Path\n",
        "\n",
        "# Caminho do seu parquet (ajuste para o do seu Drive)\n",
        "PARQUET_PATH = \"/content/drive/MyDrive/DataSet TCC/DEAM/fingerprint/0646_Backed_Vibes_Clean/peaks_all_blocks.parquet\"\n",
        "\n",
        "# Nomes de colunas \"preferidos\" (se existirem)\n",
        "COL_SONG_CANDIDATES        = [\"song_id\", \"track_id\", \"id\", \"deam_id\"]\n",
        "COL_TITLE_CANDIDATES       = [\"title\", \"song_title\", \"track_title\", \"music_title\"]\n",
        "COL_BLOCK_CANDIDATES       = [\"block_id\", \"segment_id\", \"block\"]\n",
        "\n",
        "COL_T_ANCHOR_CANDIDATES    = [\"t_anchor_s\", \"t_anchor\", \"t_s\", \"time_s\", \"time\", \"t\"]\n",
        "COL_F_ANCHOR_CANDIDATES    = [\"f_anchor_hz\", \"f_anchor\", \"f_hz\", \"freq_hz\", \"frequency\", \"f\"]\n",
        "\n",
        "COL_EMOTION_CANDIDATES     = [\"emo_quadrant\", \"emotion_quadrant\", \"emo_label\", \"emotion_label\"]\n",
        "\n",
        "COL_MAG_CANDIDATES         = [\"mag_db\", \"magnitude_db\", \"mag\"]\n",
        "\n",
        "# Metadados de STFT\n",
        "COL_STFT_NFFT_CANDIDATES   = [\"stft_nfft\", \"nfft\", \"NFFT\"]\n",
        "COL_STFT_HOP_CANDIDATES    = [\"stft_hop\", \"hop_length\", \"hop\", \"stft_hop_length\"]\n",
        "COL_STFT_WIN_CANDIDATES    = [\"stft_window\", \"window\", \"win_name\", \"stft_win\"]\n",
        "COL_SR_CANDIDATES          = [\"sr\", \"sample_rate\", \"fs\"]\n",
        "\n",
        "# Emo√ß√£o cont√≠nua\n",
        "COL_VALENCE_CANDIDATES     = [\"valence_mean\", \"valence\", \"valence_block\", \"valence_norm\"]\n",
        "COL_AROUSAL_CANDIDATES     = [\"arousal_mean\", \"arousal\", \"arousal_block\", \"arousal_norm\"]\n",
        "\n",
        "\n",
        "def _pick_first_existing(columns, candidates):\n",
        "    \"\"\"Retorna o primeiro nome de coluna que existir no DataFrame.\"\"\"\n",
        "    for c in candidates:\n",
        "        if c in columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_fingerprint(\n",
        "    parquet_path: str,\n",
        "    song_id=None,\n",
        "    block_id=None,\n",
        "    emotion=None,\n",
        "    max_points: int = 8000,\n",
        "    save_html: str | None = None\n",
        "):\n",
        "    parquet_path = Path(parquet_path)\n",
        "    print(f\"Carregando parquet: {parquet_path}\")\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "\n",
        "    cols = df.columns.tolist()\n",
        "    print(\"Colunas dispon√≠veis:\", cols)\n",
        "\n",
        "    # ---- Descobrir colunas dinamicamente ----\n",
        "    col_song     = _pick_first_existing(cols, COL_SONG_CANDIDATES)\n",
        "    col_title    = _pick_first_existing(cols, COL_TITLE_CANDIDATES)\n",
        "    col_block    = _pick_first_existing(cols, COL_BLOCK_CANDIDATES)\n",
        "    col_t_anchor = _pick_first_existing(cols, COL_T_ANCHOR_CANDIDATES)\n",
        "    col_f_anchor = _pick_first_existing(cols, COL_F_ANCHOR_CANDIDATES)\n",
        "    col_emotion  = _pick_first_existing(cols, COL_EMOTION_CANDIDATES)\n",
        "    col_mag      = _pick_first_existing(cols, COL_MAG_CANDIDATES)\n",
        "\n",
        "    col_nfft     = _pick_first_existing(cols, COL_STFT_NFFT_CANDIDATES)\n",
        "    col_hop      = _pick_first_existing(cols, COL_STFT_HOP_CANDIDATES)\n",
        "    col_window   = _pick_first_existing(cols, COL_STFT_WIN_CANDIDATES)\n",
        "    col_sr       = _pick_first_existing(cols, COL_SR_CANDIDATES)\n",
        "\n",
        "    col_valence  = _pick_first_existing(cols, COL_VALENCE_CANDIDATES)\n",
        "    col_arousal  = _pick_first_existing(cols, COL_AROUSAL_CANDIDATES)\n",
        "\n",
        "    if col_t_anchor is None or col_f_anchor is None:\n",
        "        raise ValueError(\n",
        "            \"N√£o encontrei colunas de tempo/frequ√™ncia para os picos.\\n\"\n",
        "            f\"Tente nomear as colunas como algo em {COL_T_ANCHOR_CANDIDATES} e {COL_F_ANCHOR_CANDIDATES}\"\n",
        "        )\n",
        "\n",
        "    print(f\"Usando coluna de tempo: {col_t_anchor}\")\n",
        "    print(f\"Usando coluna de frequ√™ncia: {col_f_anchor}\")\n",
        "\n",
        "    # ---- Filtros opcionais ----\n",
        "    if col_song is not None and song_id is not None:\n",
        "        df = df[df[col_song] == song_id]\n",
        "\n",
        "    if col_block is not None and block_id is not None:\n",
        "        df = df[df[col_block] == block_id]\n",
        "\n",
        "    if col_emotion is not None and emotion is not None:\n",
        "        df = df[df[col_emotion] == emotion]\n",
        "\n",
        "    df = df.dropna(subset=[col_t_anchor, col_f_anchor])\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Nenhum dado ap√≥s os filtros (song/block/emotion).\")\n",
        "\n",
        "    total_points = len(df)\n",
        "    if total_points > max_points:\n",
        "        df = df.sample(max_points, random_state=42)\n",
        "        print(f\"Amostrando {len(df)} pontos (de {total_points:,}).\")\n",
        "\n",
        "    # ---- Metadados agregados para o texto ----\n",
        "    def _safe_scalar(col):\n",
        "        if col is None or col not in df.columns:\n",
        "            return None\n",
        "        values = df[col].dropna()\n",
        "        if values.empty:\n",
        "            return None\n",
        "        return values.iloc[0]\n",
        "\n",
        "    # ID e t√≠tulo\n",
        "    song_id_str = str(_safe_scalar(col_song)) if col_song else None\n",
        "    song_title_str = str(_safe_scalar(col_title)) if col_title else None\n",
        "\n",
        "    # STFT\n",
        "    nfft_val   = _safe_scalar(col_nfft)\n",
        "    hop_val    = _safe_scalar(col_hop)\n",
        "    win_val    = _safe_scalar(col_window)\n",
        "    sr_val     = _safe_scalar(col_sr)\n",
        "\n",
        "    # Valence / arousal\n",
        "    valence_mean = df[col_valence].mean() if col_valence else None\n",
        "    arousal_mean = df[col_arousal].mean() if col_arousal else None\n",
        "\n",
        "    # Quadrante emocional predominante\n",
        "    quadrant_str = None\n",
        "    if col_emotion and not df[col_emotion].dropna().empty:\n",
        "        quadrant_str = df[col_emotion].mode().iloc[0]\n",
        "\n",
        "    # Bloco (se voc√™ filtrou por um bloco espec√≠fico, use esse; sen√£o, podemos dizer \"m√∫ltiplos blocos\")\n",
        "    block_info = None\n",
        "    if block_id is not None:\n",
        "        block_info = str(block_id)\n",
        "    elif col_block and not df[col_block].dropna().empty:\n",
        "        # se houver v√°rios, pega o primeiro s√≥ para descritivo\n",
        "        block_info = str(df[col_block].iloc[0])\n",
        "\n",
        "    # ---- Monta string principal para dentro da figura ----\n",
        "    # T√≠tulo da faixa\n",
        "    if song_title_str and song_id_str:\n",
        "        faixa_txt = f\"‚Äú{song_title_str}‚Äù (DEAM ID {song_id_str})\"\n",
        "    elif song_id_str:\n",
        "        faixa_txt = f\"DEAM ID {song_id_str}\"\n",
        "    elif song_title_str:\n",
        "        faixa_txt = f\"‚Äú{song_title_str}‚Äù\"\n",
        "    else:\n",
        "        faixa_txt = \"Faixa da base DEAM\"\n",
        "\n",
        "    stft_parts = []\n",
        "    if nfft_val is not None:\n",
        "        stft_parts.append(f\"NFFT = {int(nfft_val)}\")\n",
        "    if hop_val is not None:\n",
        "        stft_parts.append(f\"hop = {int(hop_val)}\")\n",
        "    if win_val is not None:\n",
        "        stft_parts.append(f\"janela {win_val}\")\n",
        "    if sr_val is not None:\n",
        "        stft_parts.append(f\"fs = {int(sr_val)} Hz\")\n",
        "\n",
        "    stft_txt = \", \".join(stft_parts) if stft_parts else \"par√¢metros de STFT n√£o informados\"\n",
        "\n",
        "    emo_parts = []\n",
        "    if valence_mean is not None:\n",
        "        emo_parts.append(f\"valence m√©dia ‚âà {valence_mean:.2f}\")\n",
        "    if arousal_mean is not None:\n",
        "        emo_parts.append(f\"arousal m√©dio ‚âà {arousal_mean:.2f}\")\n",
        "    if quadrant_str:\n",
        "        emo_parts.append(f\"quadrante emocional: {quadrant_str}\")\n",
        "    if block_info:\n",
        "        emo_parts.append(f\"bloco: {block_info}\")\n",
        "\n",
        "    emo_txt = \"; \".join(emo_parts) if emo_parts else \"sem metadados emocionais dispon√≠veis\"\n",
        "\n",
        "    # Texto final para desenhar na figura (multilinha)\n",
        "    text_inside = (\n",
        "        f\"{faixa_txt}\\n\"\n",
        "        f\"STFT: {stft_txt}\\n\"\n",
        "        f\"{emo_txt}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n===== TEXTO DIN√ÇMICO PARA A FIGURA =====\")\n",
        "    print(text_inside)\n",
        "    print(\"========================================\\n\")\n",
        "\n",
        "    # ---- Configura cor ----\n",
        "    if col_emotion is not None:\n",
        "        color_arg = col_emotion\n",
        "    elif col_block is not None:\n",
        "        color_arg = col_block\n",
        "    else:\n",
        "        color_arg = None\n",
        "\n",
        "    # ---- Configura tamanho ----\n",
        "    if col_mag is not None:\n",
        "        mag = df[col_mag].astype(float)\n",
        "        mag_norm = (mag - mag.min()) / (mag.max() - mag.min() + 1e-9)\n",
        "        df[\"_marker_size\"] = 4 + 10 * mag_norm\n",
        "        size_arg = \"_marker_size\"\n",
        "    else:\n",
        "        size_arg = None\n",
        "\n",
        "    # ---- Cria um √öNICO scatter ----\n",
        "    fig = px.scatter(\n",
        "        df,\n",
        "        x=col_t_anchor,\n",
        "        y=col_f_anchor,\n",
        "        color=color_arg,\n",
        "        size=size_arg,\n",
        "        labels={\n",
        "            col_t_anchor: \"Tempo (s)\",\n",
        "            col_f_anchor: \"Frequ√™ncia (Hz)\"\n",
        "        },\n",
        "        title=\"Fingerprint espectral\",\n",
        "    )\n",
        "\n",
        "    fig.update_traces(\n",
        "        mode=\"markers\",\n",
        "        marker=dict(\n",
        "            symbol=\"circle\",\n",
        "            opacity=0.7,\n",
        "            line=dict(width=0)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Texto dentro da figura (canto inferior esquerdo)\n",
        "    fig.add_annotation(\n",
        "        text=text_inside.replace(\"\\n\", \"<br>\"),  # quebra de linha no HTML\n",
        "        showarrow=False,\n",
        "        xref=\"paper\",\n",
        "        yref=\"paper\",\n",
        "        x=0.01,\n",
        "        y=-0.20,\n",
        "        align=\"left\",\n",
        "        font=dict(size=10),\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis=dict(title=\"Tempo (s)\", zeroline=False),\n",
        "        yaxis=dict(title=\"Frequ√™ncia (Hz)\", zeroline=False),\n",
        "        template=\"simple_white\",\n",
        "        legend_title_text=\"Emo√ß√£o/Bloco\" if color_arg else None,\n",
        "        height=700,\n",
        "        margin=dict(b=120)  # espa√ßo extra embaixo para o texto\n",
        "    )\n",
        "\n",
        "    if save_html is not None:\n",
        "        save_html = str(save_html)\n",
        "        fig.write_html(save_html)\n",
        "        print(f\"Figura salva em: {save_html}\")\n",
        "\n",
        "    fig.show()\n",
        "    # se n√£o quiser duplicar em alguns ambientes, pode N√ÉO retornar a figura\n",
        "    return fig\n",
        "\n",
        "\n",
        "# ================== EXEMPLO DE USO ==================\n",
        "plot_fingerprint(PARQUET_PATH, max_points=10000)\n",
        "# ou, se quiser um bloco espec√≠fico:\n",
        "# plot_fingerprint(PARQUET_PATH, block_id=3, max_points=8000)\n"
      ],
      "metadata": {
        "id": "6658GWKThkkJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13_JtK6U7FVKilzg8y2gmPg4Ni2eZEzxK",
      "authorship_tag": "ABX9TyP7e4NoVJGKcJLxVpbL7MkM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}